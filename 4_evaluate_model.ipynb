{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evauation\n",
    "Classification models need the following metrics\n",
    "\n",
    "Cross Validation\n",
    "- Accuracy\n",
    "- error\n",
    "\n",
    "ROC Curve\n",
    "Steps\n",
    "- load data\n",
    "- featurize\n",
    "- choose model\n",
    "- pass data and HP's\n",
    "- run CV\n",
    "- ROC possible?\n",
    "- tabulate results\n",
    "- report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Standard Libraries ####\n",
    "import os\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "import timeit\n",
    "import uuid\n",
    "\n",
    "#### third-party Libraries ####\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier as SRC\n",
    "from lolopy.learners import RandomForestClassifier as LRC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "#### Local Libraries ####\n",
    "from utils import (Result, run_k_folds, \n",
    "                   report_column_labels,\n",
    "                   compile_data)\n",
    "from data_manager import DataManager\n",
    "from featurizer import Featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "np.random.seed(8)\n",
    "load_path = os.path.join('data','training_data.csv')\n",
    "save_path = os.path.join('results','baseline.csv')\n",
    "mp_api_key = '7n6DwPUQ5cf8ZTWO'\n",
    "oversample = False\n",
    "data_ramp = False\n",
    "feature_set = ['standard']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "This object should handle data load and sampling.\n",
    "\n",
    "Should be able to do the following:\n",
    "1. load and sample \n",
    "2. Run a data ramp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Loaded 2572 records.'\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "dm = DataManager(load_path, save_path)\n",
    "dm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "dm.sample_data(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format and careate composition objects\n",
    "#dm.compute_formula()\n",
    "dm.to_binary_classes()\n",
    "dm.get_pymatgen_composition()\n",
    "dm.remove_noble_gasses()\n",
    "dm.remove_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Featurizer(feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852fbbab110b4490881a21657d29f7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='MultipleFeaturizer', max=871, style=ProgressStyle(descriptionâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dm.featurized_data = f.featurize(dm.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.groups = dm.data['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training labels\n",
    "dm.outputs = dm.data['stable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds = partial(run_k_folds, inputs=dm.featurized_data, outputs=dm.outputs, groups=dm.groups, sampling=oversample, ramp=data_ramp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set models\n",
    "models = [GaussianNB(), SVC(), SRC(), LogisticRegression(), DummyClassifier(strategy=\"most_frequent\")]\n",
    "#models = [GaussianNB()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pool = mp.Pool(processes=mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "start_time = timeit.default_timer()\n",
    "results = pool.map(k_folds, models)    \n",
    "elapsed = timeit.default_timer() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>data_size</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy_std</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>recall</th>\n",
       "      <th>recall_std</th>\n",
       "      <th>precision</th>\n",
       "      <th>precision_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>871</td>\n",
       "      <td>0.776015</td>\n",
       "      <td>0.083716</td>\n",
       "      <td>0.449831</td>\n",
       "      <td>0.070163</td>\n",
       "      <td>0.539860</td>\n",
       "      <td>0.128882</td>\n",
       "      <td>0.411505</td>\n",
       "      <td>0.101495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>871</td>\n",
       "      <td>0.833208</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>871</td>\n",
       "      <td>0.915012</td>\n",
       "      <td>0.022541</td>\n",
       "      <td>0.673610</td>\n",
       "      <td>0.080569</td>\n",
       "      <td>0.532389</td>\n",
       "      <td>0.088506</td>\n",
       "      <td>0.931623</td>\n",
       "      <td>0.076861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>871</td>\n",
       "      <td>0.874861</td>\n",
       "      <td>0.056559</td>\n",
       "      <td>0.636372</td>\n",
       "      <td>0.113778</td>\n",
       "      <td>0.627497</td>\n",
       "      <td>0.108862</td>\n",
       "      <td>0.672184</td>\n",
       "      <td>0.158903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>871</td>\n",
       "      <td>0.833208</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     type  data_size  accuracy  accuracy_std        f1  \\\n",
       "0              GaussianNB        871  0.776015      0.083716  0.449831   \n",
       "1                     SVC        871  0.833208      0.030400  0.000000   \n",
       "2  RandomForestClassifier        871  0.915012      0.022541  0.673610   \n",
       "3      LogisticRegression        871  0.874861      0.056559  0.636372   \n",
       "4         DummyClassifier        871  0.833208      0.030400  0.000000   \n",
       "\n",
       "     f1_std    recall  recall_std  precision  precision_std  \n",
       "0  0.070163  0.539860    0.128882   0.411505       0.101495  \n",
       "1  0.000000  0.000000    0.000000   0.000000       0.000000  \n",
       "2  0.080569  0.532389    0.088506   0.931623       0.076861  \n",
       "3  0.113778  0.627497    0.108862   0.672184       0.158903  \n",
       "4  0.000000  0.000000    0.000000   0.000000       0.000000  "
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled = []\n",
    "for result in results:\n",
    "    compiled.extend(compile_data(result))\n",
    "res_df = pd.DataFrame(compiled, columns=report_column_labels)\n",
    "res_df.to_csv(save_path)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from sklearn.utils import resample\n",
    "from collections import Counter\n",
    "\n",
    "def local_oversample(train, outputs):\n",
    "    majority_class, minority_class = Counter(outputs[train]).most_common()\n",
    "    minority = train[numpy.where(outputs[train] == minority_class[0])[0]]\n",
    "    majority = train[numpy.where(outputs[train] == majority_class[0])[0]]\n",
    "    minority_upsampled = resample(minority,\n",
    "                              replace=True,\n",
    "                              n_samples=len(majority),\n",
    "                              random_state=8)\n",
    "    t = numpy.append(majority, minority_upsampled)\n",
    "    pprint(len(t))\n",
    "    return (t, minority, majority, minority_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SRC()\n",
    "ovs = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import KFold, GroupKFold, LeavePGroupsOut, LeaveOneGroupOut\n",
    "inputs = dm.featurized_data\n",
    "outputs = dm.outputs\n",
    "groups = dm.groups\n",
    "gkf = GroupKFold(n_splits=10)\n",
    "lpgo = LeavePGroupsOut(n_groups=1)\n",
    "logo = LeaveOneGroupOut()\n",
    "splits = list(gkf.split(inputs, outputs, groups=groups.values))\n",
    "#splits = list(logo.split(inputs, outputs, groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = splits[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Place all data into df's for easy analysis\n",
    "train_df = pandas.DataFrame(dm.data.ix[train, ['formula','group','stable']])\n",
    "test_df = pandas.DataFrame(dm.data.ix[test, ['formula','group','stable']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ovs:\n",
    "    train, minority, majority, minority_upsampled = local_oversample(train, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdavidson/.conda/envs/tecca/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning:\n",
      "\n",
      "The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the ML\n",
    "model.fit(inputs[train], outputs[train])\n",
    "prediction = model.predict(inputs[test])\n",
    "acc = accuracy_score(outputs[test], prediction)\n",
    "f1s = [precision_recall_fscore_support(\n",
    "    outputs[test], prediction, labels=[0,1], average='binary')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Place all data into df's for easy analysis\n",
    "train_df = pandas.DataFrame(dm.data.ix[train, ['formula','group','stable']])\n",
    "test_df = pandas.DataFrame(dm.data.ix[test, ['formula','group','stable']])\n",
    "pred_df = pandas.DataFrame(dm.data.ix[test, ['formula','group','stable']])\n",
    "pred_df['prediction'] = prediction\n",
    "acc_df = pandas.DataFrame([acc], columns=['accuracy'])\n",
    "\n",
    "f1_df = pandas.DataFrame(\n",
    "        f1s, columns=['precision', 'recall', 'f1', 'support'])\n",
    "ov_df = pd.DataFrame([train_df['group'].isin(test_df['group']).any()], columns=['overlap'])\n",
    "\n",
    "try:\n",
    "    lab_df = pandas.DataFrame([pred_df['prediction'].value_counts().values], columns=['pred_0','pred_1'])\n",
    "except:\n",
    "    lab_df = pandas.DataFrame([pred_df['prediction'].value_counts().values], columns=['pred_0'])\n",
    "if ovs:\n",
    "    sample_df = pandas.DataFrame([train_df['stable'].value_counts().values], columns=['smpl_0','smpl_1'])\n",
    "else:\n",
    "    sample_df = pandas.DataFrame([], columns=['sample'])\n",
    "# Get the accuracy for each label\n",
    "#ac_lab_df = pandas.DataFrame([test_df['stable'].value_counts().values], columns=['acc_0','acc_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res = f1_df.join(acc_df).join(ov_df).join(lab_df).join(ac_lab_df).join(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = f1_df.join(acc_df).join(ov_df).join(lab_df).join(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res.to_csv('results/final_modle.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>support</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>overlap</th>\n",
       "      <th>pred_0</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.697531</td>\n",
       "      <td>0.43295</td>\n",
       "      <td>0.534279</td>\n",
       "      <td>None</td>\n",
       "      <td>0.904229</td>\n",
       "      <td>False</td>\n",
       "      <td>1895</td>\n",
       "      <td>162</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision   recall        f1 support  accuracy  overlap  pred_0  pred_1  \\\n",
       "0   0.697531  0.43295  0.534279    None  0.904229    False    1895     162   \n",
       "\n",
       "  sample  \n",
       "0    NaN  "
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#pickle.dump(model, open('rfc.sav', 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
